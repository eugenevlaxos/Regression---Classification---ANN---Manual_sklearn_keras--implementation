{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Regression - Classification - ANN -  Manual_sklearn_keras- implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We first begin with the classification of cities_USA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88108841, 0.        ],\n",
       "       [0.88055904, 0.00594496],\n",
       "       [0.88178569, 0.01279241],\n",
       "       [0.88021494, 0.01415109],\n",
       "       [0.86448345, 0.01547544],\n",
       "       [0.88207643, 0.01798893],\n",
       "       [0.86377452, 0.02396688],\n",
       "       [0.86456117, 0.04846615],\n",
       "       [0.85498638, 0.04866702],\n",
       "       [0.7049403 , 0.04917107],\n",
       "       [0.70536344, 0.05035016],\n",
       "       [0.85455771, 0.05162711],\n",
       "       [0.86481561, 0.06748655],\n",
       "       [0.86448325, 0.0711972 ],\n",
       "       [0.68792987, 0.07550291],\n",
       "       [0.86600741, 0.09124038],\n",
       "       [0.77330254, 0.09290447],\n",
       "       [0.68101261, 0.0935747 ],\n",
       "       [0.72912018, 0.09361724],\n",
       "       [0.72600489, 0.0951411 ],\n",
       "       [0.70422131, 0.10391652],\n",
       "       [0.70152241, 0.10470923],\n",
       "       [0.71539169, 0.10582105],\n",
       "       [0.77353257, 0.10804416],\n",
       "       [0.70175804, 0.10924092],\n",
       "       [0.79927945, 0.11448004],\n",
       "       [0.68793629, 0.11993111],\n",
       "       [0.83993259, 0.12269252],\n",
       "       [0.78217988, 0.12339899],\n",
       "       [0.84681247, 0.12540482],\n",
       "       [0.69999194, 0.12819766],\n",
       "       [0.7937793 , 0.1294605 ],\n",
       "       [0.75271656, 0.13057214],\n",
       "       [0.75623859, 0.13348513],\n",
       "       [0.67540315, 0.13394602],\n",
       "       [0.85979392, 0.13473249],\n",
       "       [0.84704782, 0.13588719],\n",
       "       [0.64402996, 0.13617469],\n",
       "       [0.72516656, 0.13696039],\n",
       "       [0.735517  , 0.1373078 ],\n",
       "       [0.84952022, 0.13921905],\n",
       "       [0.71666693, 0.14015872],\n",
       "       [0.71444877, 0.14123489],\n",
       "       [0.85499297, 0.14282639],\n",
       "       [0.76497964, 0.14383447],\n",
       "       [0.64604361, 0.15148444],\n",
       "       [0.82480692, 0.15274516],\n",
       "       [0.56677058, 0.15386869],\n",
       "       [0.56661661, 0.15517138],\n",
       "       [0.5780583 , 0.15762244],\n",
       "       [0.83226129, 0.15883833],\n",
       "       [0.84436063, 0.16409655],\n",
       "       [0.78974366, 0.16668801],\n",
       "       [0.70446147, 0.16673631],\n",
       "       [0.81598141, 0.16674325],\n",
       "       [0.50387961, 0.1671859 ],\n",
       "       [0.71129402, 0.16747166],\n",
       "       [0.77984585, 0.16810832],\n",
       "       [0.71227766, 0.16951333],\n",
       "       [0.71189743, 0.17161336],\n",
       "       [0.84773802, 0.17364005],\n",
       "       [0.85228104, 0.17868019],\n",
       "       [0.50950719, 0.18495952],\n",
       "       [0.55669937, 0.18537881],\n",
       "       [0.66458198, 0.18843502],\n",
       "       [0.8638363 , 0.18878372],\n",
       "       [0.49552562, 0.18918492],\n",
       "       [0.5506615 , 0.19065314],\n",
       "       [0.83519273, 0.19065329],\n",
       "       [0.85305617, 0.19090323],\n",
       "       [0.73587481, 0.1926663 ],\n",
       "       [0.49361619, 0.1941637 ],\n",
       "       [0.7244609 , 0.19627646],\n",
       "       [0.49247886, 0.19705345],\n",
       "       [0.88487011, 0.19824016],\n",
       "       [0.49477004, 0.19913241],\n",
       "       [0.49034251, 0.1995811 ],\n",
       "       [0.86827469, 0.19996241],\n",
       "       [0.69244968, 0.20606415],\n",
       "       [0.50091402, 0.20979033],\n",
       "       [0.90307674, 0.21065376],\n",
       "       [0.7808518 , 0.2123439 ],\n",
       "       [0.68930955, 0.21259861],\n",
       "       [0.83741362, 0.21426317],\n",
       "       [0.83461553, 0.21600184],\n",
       "       [0.86480239, 0.21728302],\n",
       "       [0.8687386 , 0.21744081],\n",
       "       [0.82786125, 0.21803337],\n",
       "       [0.51975469, 0.21884233],\n",
       "       [0.87726485, 0.2200214 ],\n",
       "       [0.75227275, 0.22061021],\n",
       "       [0.7808961 , 0.22191637],\n",
       "       [0.91284274, 0.22561103],\n",
       "       [0.68939199, 0.22617986],\n",
       "       [0.85723913, 0.22713325],\n",
       "       [0.85727418, 0.23021633],\n",
       "       [0.70524099, 0.23050152],\n",
       "       [0.83609731, 0.23333142],\n",
       "       [0.64673726, 0.23418169],\n",
       "       [0.6864159 , 0.23455151],\n",
       "       [0.86535831, 0.23693304],\n",
       "       [0.62446474, 0.23901084],\n",
       "       [0.70678601, 0.23954055],\n",
       "       [0.85198454, 0.24043395],\n",
       "       [0.86488131, 0.24542689],\n",
       "       [0.81266344, 0.24582415],\n",
       "       [0.9060809 , 0.24785077],\n",
       "       [0.77796115, 0.24949865],\n",
       "       [0.92046302, 0.25389638],\n",
       "       [0.8295243 , 0.25414706],\n",
       "       [0.87650331, 0.2550091 ],\n",
       "       [0.63212466, 0.25601544],\n",
       "       [0.86048054, 0.25777785],\n",
       "       [0.83460191, 0.2602333 ],\n",
       "       [0.9204436 , 0.26239714],\n",
       "       [0.45434974, 0.26612999],\n",
       "       [0.84791881, 0.2668747 ],\n",
       "       [0.8594682 , 0.26691606],\n",
       "       [0.77873783, 0.26988108],\n",
       "       [0.77093498, 0.27244576],\n",
       "       [0.86544704, 0.2737611 ],\n",
       "       [0.8856288 , 0.27638966],\n",
       "       [0.85364449, 0.27673249],\n",
       "       [0.85416765, 0.27709153],\n",
       "       [0.46748997, 0.27746302],\n",
       "       [0.91977559, 0.27804367],\n",
       "       [0.85519597, 0.27933013],\n",
       "       [0.8944708 , 0.27969958],\n",
       "       [0.8391299 , 0.28047084],\n",
       "       [0.85674701, 0.28078911],\n",
       "       [0.85429929, 0.28211103],\n",
       "       [0.72333515, 0.28215677],\n",
       "       [0.70643923, 0.28257215],\n",
       "       [0.79858236, 0.28319027],\n",
       "       [0.45260863, 0.28378626],\n",
       "       [0.85450741, 0.28381881],\n",
       "       [0.89209856, 0.28447824],\n",
       "       [0.89232575, 0.28660478],\n",
       "       [0.69747273, 0.28707343],\n",
       "       [0.44918201, 0.28708174],\n",
       "       [0.86363931, 0.28723193],\n",
       "       [0.90691833, 0.28811087],\n",
       "       [0.92997118, 0.2886751 ],\n",
       "       [0.47221037, 0.2897051 ],\n",
       "       [0.79170987, 0.29038083],\n",
       "       [0.86075539, 0.29059076],\n",
       "       [0.78156451, 0.29256555],\n",
       "       [0.46220059, 0.29510966],\n",
       "       [0.84302452, 0.29544773],\n",
       "       [0.56835917, 0.29632796],\n",
       "       [0.92267684, 0.29722089],\n",
       "       [0.79900685, 0.29898131],\n",
       "       [0.85669952, 0.30061731],\n",
       "       [0.85774464, 0.30111826],\n",
       "       [0.68908939, 0.3016059 ],\n",
       "       [0.71684065, 0.30240913],\n",
       "       [0.68686599, 0.30263395],\n",
       "       [0.91558207, 0.30266651],\n",
       "       [0.74779111, 0.30266684],\n",
       "       [0.46909839, 0.30521345],\n",
       "       [0.78882475, 0.30556423],\n",
       "       [0.81912293, 0.30616595],\n",
       "       [0.79753586, 0.3061769 ],\n",
       "       [0.80260375, 0.30676685],\n",
       "       [0.92164141, 0.30757665],\n",
       "       [0.77742402, 0.30780437],\n",
       "       [0.78995979, 0.30819059],\n",
       "       [0.80692594, 0.30822426],\n",
       "       [0.8332667 , 0.30880052],\n",
       "       [0.78254043, 0.30891268],\n",
       "       [0.73343763, 0.3116575 ],\n",
       "       [0.77733151, 0.31306127],\n",
       "       [0.87046143, 0.31447983],\n",
       "       [0.83612321, 0.31473829],\n",
       "       [0.76475546, 0.31577884],\n",
       "       [0.87373033, 0.31601241],\n",
       "       [0.93552685, 0.31636753],\n",
       "       [0.7338861 , 0.31639172],\n",
       "       [0.91376469, 0.31683725],\n",
       "       [0.93680642, 0.31739372],\n",
       "       [0.78594031, 0.31749948],\n",
       "       [0.90599522, 0.31752478],\n",
       "       [0.90219161, 0.31862896],\n",
       "       [0.68906746, 0.31872465],\n",
       "       [0.91012301, 0.31892993],\n",
       "       [0.75504452, 0.32118258],\n",
       "       [0.76695129, 0.32165871],\n",
       "       [0.91934168, 0.32323032],\n",
       "       [0.89606059, 0.3248979 ],\n",
       "       [0.89049857, 0.32521322],\n",
       "       [0.91779663, 0.32593557],\n",
       "       [0.62812799, 0.32769184],\n",
       "       [0.90583289, 0.32888574],\n",
       "       [0.87693583, 0.32952708],\n",
       "       [0.78706529, 0.32957469],\n",
       "       [0.4722903 , 0.329616  ],\n",
       "       [0.62885593, 0.33000175],\n",
       "       [0.87963317, 0.33053844],\n",
       "       [0.92004658, 0.33171059],\n",
       "       [0.73858028, 0.33183528],\n",
       "       [0.77170667, 0.33193348],\n",
       "       [0.90756586, 0.33198781],\n",
       "       [0.69689951, 0.33245389],\n",
       "       [0.68761173, 0.33293618],\n",
       "       [0.62700694, 0.33352297],\n",
       "       [0.78695345, 0.33396248],\n",
       "       [0.77383976, 0.3349318 ],\n",
       "       [0.76962872, 0.3349528 ],\n",
       "       [0.94201907, 0.33555518],\n",
       "       [0.80182519, 0.33565215],\n",
       "       [0.93035463, 0.33590651],\n",
       "       [0.76367502, 0.33668241],\n",
       "       [0.83456627, 0.33682928],\n",
       "       [0.8308362 , 0.33688836],\n",
       "       [0.92186236, 0.33805713],\n",
       "       [0.93215939, 0.33890828],\n",
       "       [0.92029362, 0.33934595],\n",
       "       [0.85704115, 0.33942641],\n",
       "       [0.68943478, 0.34004882],\n",
       "       [0.93793386, 0.34131694],\n",
       "       [0.62664575, 0.34176519],\n",
       "       [0.93225087, 0.34186277],\n",
       "       [0.73168774, 0.34195995],\n",
       "       [0.7897546 , 0.34331648],\n",
       "       [0.88563184, 0.34335923],\n",
       "       [0.94113864, 0.3433661 ],\n",
       "       [0.90743998, 0.34478105],\n",
       "       [0.66716609, 0.34555771],\n",
       "       [0.89428109, 0.34604649],\n",
       "       [0.79971596, 0.34640866],\n",
       "       [0.85893936, 0.34737791],\n",
       "       [0.89036426, 0.34797822],\n",
       "       [0.90310044, 0.34941768],\n",
       "       [0.88494536, 0.3504021 ],\n",
       "       [0.92368633, 0.35051535],\n",
       "       [0.67551904, 0.3512175 ],\n",
       "       [0.89971101, 0.35181629],\n",
       "       [0.78622294, 0.35207183],\n",
       "       [0.94383989, 0.35213482],\n",
       "       [0.93875492, 0.35247329],\n",
       "       [0.94765642, 0.35322728],\n",
       "       [0.68630027, 0.35431211],\n",
       "       [0.94394976, 0.35432075],\n",
       "       [0.94178906, 0.35433533],\n",
       "       [0.94362771, 0.35513254],\n",
       "       [0.95380655, 0.3552644 ],\n",
       "       [0.89769081, 0.35526992],\n",
       "       [0.68122505, 0.35608811],\n",
       "       [0.43354365, 0.35696999],\n",
       "       [0.82554172, 0.35789982],\n",
       "       [0.93262212, 0.35882106],\n",
       "       [0.94024347, 0.3589172 ],\n",
       "       [0.94303153, 0.35891753],\n",
       "       [0.94561626, 0.35898678],\n",
       "       [0.94090714, 0.35929127],\n",
       "       [0.94811047, 0.36240987],\n",
       "       [0.96093421, 0.36267162],\n",
       "       [0.5059445 , 0.36520743],\n",
       "       [0.69248254, 0.36635997],\n",
       "       [0.857337  , 0.36694142],\n",
       "       [0.91627144, 0.36719927],\n",
       "       [0.8584821 , 0.3673832 ],\n",
       "       [0.95971296, 0.36806537],\n",
       "       [0.94572483, 0.36891345],\n",
       "       [0.87719517, 0.3690569 ],\n",
       "       [0.7185817 , 0.36908559],\n",
       "       [0.72125954, 0.36975738],\n",
       "       [0.86426054, 0.3705493 ],\n",
       "       [0.8668915 , 0.37298574],\n",
       "       [0.88921131, 0.37323118],\n",
       "       [0.91945436, 0.37350478],\n",
       "       [0.71449737, 0.37350667],\n",
       "       [0.95722999, 0.3737615 ],\n",
       "       [0.96350201, 0.37383136],\n",
       "       [0.79981654, 0.37393548],\n",
       "       [0.91037559, 0.37407826],\n",
       "       [0.97843155, 0.37448487],\n",
       "       [0.80660521, 0.37613966],\n",
       "       [0.84482258, 0.37689162],\n",
       "       [0.79760051, 0.37731932],\n",
       "       [0.84365941, 0.3776496 ],\n",
       "       [0.81905103, 0.37768918],\n",
       "       [0.97490091, 0.37917284],\n",
       "       [0.80411367, 0.37951951],\n",
       "       [0.7810343 , 0.3804282 ],\n",
       "       [0.72623193, 0.38067355],\n",
       "       [0.92644362, 0.38160184],\n",
       "       [0.90793712, 0.38284411],\n",
       "       [0.93248291, 0.38438481],\n",
       "       [0.67045292, 0.38452056],\n",
       "       [0.93652684, 0.38512554],\n",
       "       [0.80432172, 0.38546665],\n",
       "       [0.8817917 , 0.38705581],\n",
       "       [0.83596815, 0.38717245],\n",
       "       [0.88175025, 0.38742752],\n",
       "       [0.88186857, 0.3877255 ],\n",
       "       [0.80148718, 0.38852485],\n",
       "       [0.96739182, 0.39017361],\n",
       "       [0.9584394 , 0.3903579 ],\n",
       "       [0.73685943, 0.39128215],\n",
       "       [0.94754787, 0.3919174 ],\n",
       "       [0.92159835, 0.39200701],\n",
       "       [0.82019167, 0.39256817],\n",
       "       [0.9737879 , 0.39299239],\n",
       "       [0.88826692, 0.39385569],\n",
       "       [0.97451875, 0.39451571],\n",
       "       [0.74431697, 0.39563895],\n",
       "       [0.71566415, 0.39632055],\n",
       "       [0.91984741, 0.39654474],\n",
       "       [0.8527633 , 0.39663097],\n",
       "       [0.951628  , 0.3975402 ],\n",
       "       [0.70262057, 0.39885818],\n",
       "       [0.556076  , 0.39934538],\n",
       "       [0.84903449, 0.39960431],\n",
       "       [0.97152764, 0.39972143],\n",
       "       [0.8466298 , 0.40027801],\n",
       "       [0.75682602, 0.40063505],\n",
       "       [0.9734593 , 0.40085791],\n",
       "       [0.56705084, 0.4015715 ],\n",
       "       [0.82394935, 0.40181413],\n",
       "       [0.72988365, 0.4031186 ],\n",
       "       [0.78435357, 0.40396656],\n",
       "       [0.64812899, 0.40408273],\n",
       "       [0.85072877, 0.40448747],\n",
       "       [0.74790996, 0.40546559],\n",
       "       [0.96767187, 0.40593075],\n",
       "       [0.91462483, 0.40609397],\n",
       "       [0.43260887, 0.40799399],\n",
       "       [0.97679346, 0.4114361 ],\n",
       "       [0.92792011, 0.41182228],\n",
       "       [0.90734535, 0.41188775],\n",
       "       [0.89728986, 0.41331515],\n",
       "       [0.96604854, 0.41550754],\n",
       "       [0.4553849 , 0.41560375],\n",
       "       [0.69561248, 0.41580323],\n",
       "       [0.75281962, 0.41759197],\n",
       "       [0.72212931, 0.41794747],\n",
       "       [0.73837341, 0.41859958],\n",
       "       [0.56884347, 0.41921689],\n",
       "       [0.7983397 , 0.4226013 ],\n",
       "       [0.96800581, 0.42293656],\n",
       "       [0.75151666, 0.4231496 ],\n",
       "       [0.98201055, 0.42325398],\n",
       "       [0.95481879, 0.42494171],\n",
       "       [0.9848235 , 0.42566755],\n",
       "       [0.70542429, 0.42570143],\n",
       "       [0.97211547, 0.42936468],\n",
       "       [0.95313745, 0.42962112],\n",
       "       [0.72021918, 0.42996529],\n",
       "       [0.98513668, 0.4305546 ],\n",
       "       [0.72922196, 0.43143264],\n",
       "       [0.95702107, 0.43393514],\n",
       "       [0.96417364, 0.43424822],\n",
       "       [0.95343385, 0.43538517],\n",
       "       [0.99405767, 0.43639391],\n",
       "       [0.74424225, 0.43721988],\n",
       "       [0.71916752, 0.43761035],\n",
       "       [0.99771879, 0.44080223],\n",
       "       [0.9861651 , 0.44399834],\n",
       "       [0.47314818, 0.44528726],\n",
       "       [0.99393097, 0.44843325],\n",
       "       [0.60762929, 0.45045712],\n",
       "       [0.50625589, 0.4538939 ],\n",
       "       [0.7473726 , 0.45514301],\n",
       "       [0.74417353, 0.45532598],\n",
       "       [0.71376335, 0.45605533],\n",
       "       [0.70475033, 0.45745676],\n",
       "       [0.99521075, 0.46144765],\n",
       "       [0.75319986, 0.46312418],\n",
       "       [1.        , 0.46381015],\n",
       "       [0.80142597, 0.46986333],\n",
       "       [0.74762715, 0.47128163],\n",
       "       [0.74803125, 0.47246209],\n",
       "       [0.44928621, 0.47276608],\n",
       "       [0.80080805, 0.47358974],\n",
       "       [0.74810745, 0.47376865],\n",
       "       [0.68101051, 0.47925522],\n",
       "       [0.72261224, 0.47948582],\n",
       "       [0.72602307, 0.48330023],\n",
       "       [0.83971745, 0.48584423],\n",
       "       [0.75183449, 0.48608323],\n",
       "       [0.82427669, 0.48613381],\n",
       "       [0.82647809, 0.48723646],\n",
       "       [0.83618209, 0.48882555],\n",
       "       [0.50351519, 0.49304826],\n",
       "       [0.79682862, 0.49730231],\n",
       "       [0.77455931, 0.4991017 ],\n",
       "       [0.76023649, 0.49930121],\n",
       "       [0.71434635, 0.50438309],\n",
       "       [0.50101104, 0.50795152],\n",
       "       [0.64507731, 0.51317255],\n",
       "       [0.66655042, 0.51357578],\n",
       "       [0.66731304, 0.51782655],\n",
       "       [0.48446158, 0.52457684],\n",
       "       [0.6568279 , 0.5252734 ],\n",
       "       [0.7046001 , 0.53162287],\n",
       "       [0.71387813, 0.54026969],\n",
       "       [0.        , 0.66520742],\n",
       "       [0.02842747, 0.99029059],\n",
       "       [0.21524614, 1.        ]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "datas=pd.read_csv(\"cities_USA.csv\")\n",
    "warnings.filterwarnings('ignore')\n",
    "datas = datas.drop(\"col\",axis=1)\n",
    "\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "\n",
    "datas.iloc[:,2] = labelencoder.fit_transform(datas.iloc[:,2])\n",
    "\n",
    "corr = datas.corr()\n",
    "corr\n",
    "inputs=datas.iloc[:,:-1].values\n",
    "target=datas.iloc[:,-1].values\n",
    "scaler = MinMaxScaler()\n",
    "inputs=scaler.fit_transform(inputs)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression function\n",
    "\n",
    "def linear(input, weights, biases):\n",
    "    return np.dot(input, weights.T) + biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def activation(x):\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_deriv(x):\n",
    "    return 1-np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of our numpy model is: 0.7825\n"
     ]
    }
   ],
   "source": [
    "# hidden layer weights\n",
    "from sklearn.metrics import accuracy_score\n",
    "target=target.reshape(-1,1)\n",
    "\n",
    "learning_rate=0.001\n",
    "import numpy as np\n",
    "hidden_layer_weights = np.array([\n",
    "    [0.7, 0.2],    # unit 1\n",
    "    [0.3, 0.4]  # unit 2\n",
    "])\n",
    "hidden_layer_biases = np.array([1. ,1.])\n",
    "\n",
    "# initial output layer weights\n",
    "output_weights = np.array([[1., 1.]])\n",
    "output_biases = np.array([1.])\n",
    "\n",
    "for i in range(10000):\n",
    "    \n",
    "    hidden_linout = linear(inputs, hidden_layer_weights, hidden_layer_biases)\n",
    "    hidden_output = activation(hidden_linout)\n",
    "    \n",
    "    output_linout = linear(hidden_output, output_weights, output_biases)\n",
    "    output_output = output_linout # no activation function on output layer\n",
    "\n",
    "    predicted = output_output\n",
    "\n",
    "    # derivative of mean squared error\n",
    "    error = predicted - target\n",
    "\n",
    "    # delta for the output layer (no activation on output layer)\n",
    "    delta_output = error\n",
    "\n",
    "    # output layer updates\n",
    "    output_weights_update = delta_output.T.dot(hidden_output)\n",
    "    output_biases_update = delta_output.sum(axis = 0)\n",
    "    \n",
    "    #print(delta_output.shape)\n",
    "    #print(output_weights.shape)\n",
    "    #print(activation_deriv(hidden_linout).shape)\n",
    "        \n",
    "    # push back the delta to the hidden layer\n",
    "    delta_hidden = delta_output*output_weights*activation_deriv(hidden_linout)\n",
    "\n",
    "    # hidden layer updates\n",
    "    hidden_weights_update = delta_hidden.T.dot(inputs)\n",
    "    hidden_biases_update = delta_hidden.sum(axis = 0)\n",
    "\n",
    "    output_weights -= output_weights_update*learning_rate\n",
    "    output_biases -= output_biases_update*learning_rate\n",
    "\n",
    "    hidden_layer_weights -= hidden_weights_update*learning_rate\n",
    "    hidden_layer_biases -= hidden_biases_update*learning_rate\n",
    " \n",
    "predicted=(predicted>0.5).astype(int)\n",
    "\n",
    "score1=accuracy_score(predicted,target)\n",
    "print(\"The accuracy of our numpy model is:\",score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error\n",
      "0.2175\n"
     ]
    }
   ],
   "source": [
    "mse = np.mean((target - predicted)**2)\n",
    "print('mean squared error')\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, target,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predictions, targets):\n",
    "    import numpy as np\n",
    "    differences = predictions - targets      \n",
    "\n",
    "    differences_squared = differences ** 2                    \n",
    "\n",
    "    mean_of_differences_squared = differences_squared.mean()  \n",
    "\n",
    "    rmse_val = np.sqrt(mean_of_differences_squared)           \n",
    "\n",
    "    return rmse_val   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of hidden layers: 1\n",
      "number of hidden layers: 5\n",
      "number of hidden layers: 10\n",
      "number of hidden layers: 20\n",
      "number of hidden layers: 50\n",
      "[0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1\n",
      " 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
      " 1 1 1 1 0 0]\n",
      "scores of cross validation:  [0.778125, 0.85625, 0.9, 0.91875, 0.9125]\n",
      "accuracy of mlp classification 0.8731249999999999\n",
      "rmse for mlp classification: 0.685376247602783\n",
      "mae for mlp classification: 0.22999999999999998\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "rmse_mlp=[] \n",
    "mae_mlp=[]\n",
    "accuracy=[]\n",
    "\n",
    "list_1 = [1,5,10,20,50]\n",
    "    #if we use the sigmoid function as our  activation function, we do not have an efficient model.\n",
    "    #the same for tanh\n",
    "    #as learning rate i used the default value, and max_iter=700\n",
    "    #if we choose as solver=lbfgs we take imbalanced results from negative to too good(75%)\n",
    "for i in list_1:\n",
    "    print(\"number of hidden layers:\",i)\n",
    "    regr = MLPClassifier(hidden_layer_sizes=i, activation='tanh', solver='lbfgs',alpha=0.01,learning_rate_init=0.001,max_iter=50000)\n",
    "    regr.fit(X_train, y_train)\n",
    "    scores = cross_val_score(regr, X_train, y_train, cv=5)\n",
    "    accuracy.append(scores.mean())\n",
    "    y_pr = regr.predict((X_test))\n",
    "    error_rmse=rmse(y_pr, y_test)\n",
    "    rmse_mlp.append(error_rmse)\n",
    "    error_mae=mean_absolute_error(y_test, y_pr)\n",
    "    mae_mlp.append(error_mae)\n",
    "\n",
    "print(y_pr)\n",
    "print(\"scores of cross validation: \",accuracy) \n",
    "print(\"accuracy of mlp classification\",sum(accuracy)/len(accuracy))    \n",
    "print(\"rmse for mlp classification:\",sum(rmse_mlp) / len(rmse_mlp)) \n",
    "print(\"mae for mlp classification:\",sum(mae_mlp) / len(mae_mlp))\n",
    "score2=sum(accuracy)/len(accuracy)\n",
    "    #y_predict = regr.predict(x_test)\n",
    "    #score5 = metrics.r2_score(y_test,y_predict)\n",
    "    #score5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer = 'adam',loss = 'mean_squared_error', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=10, verbose=0)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of our numpy model is: 0.7375\n"
     ]
    }
   ],
   "source": [
    "predictions=(predictions>0.5).astype(int)\n",
    "\n",
    "score3=accuracy_score(predictions,y_test)\n",
    "print(\"The accuracy of our numpy model is:\",score3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf = clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8625\n"
     ]
    }
   ],
   "source": [
    "score4=accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\",score4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We concluded that by applying all these different techiques the accuracies of our 4 models varied.First we built a numpy ann model using backpropagation.The accuracy that we took was close to 78% and if our prediction was higher than 0.5 we decided that our predicted value was 1 and respectively 0 if our predicted value was smaller or equal than 0.5.\n",
    "# Moreover, using the sklearn library for our mlp classifier, we ended up that our average accuracy of our model was 87% with 5 fold cross validation and for 5 different sizes of hidden layers.\n",
    "# Subsequently, we created a sequential model using keras and we got an accuracy of 74% with 2 hidden layers and 10 and 5 neurons respectively.\n",
    "# And finally, by implementing decision trees classifier in our dataset, we observed that our accuracy was very high and it reached 86%.\n",
    "# So mlp classifier gave the better performance out of 4 models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We continue by applying regression models on kc_house dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     id     price  bedrooms  bathrooms  sqft_living  sqft_lot  \\\n",
      "id             1.000000 -0.016797  0.001286   0.005160    -0.012258 -0.132109   \n",
      "price         -0.016797  1.000000  0.308338   0.525134     0.702044  0.089655   \n",
      "bedrooms       0.001286  0.308338  1.000000   0.515884     0.576671  0.031703   \n",
      "bathrooms      0.005160  0.525134  0.515884   1.000000     0.754665  0.087740   \n",
      "sqft_living   -0.012258  0.702044  0.576671   0.754665     1.000000  0.172826   \n",
      "sqft_lot      -0.132109  0.089655  0.031703   0.087740     0.172826  1.000000   \n",
      "floors         0.018525  0.256786  0.175429   0.500653     0.353949 -0.005201   \n",
      "waterfront    -0.002721  0.266331 -0.006582   0.063744     0.103818  0.021604   \n",
      "view           0.011592  0.397346  0.079532   0.187737     0.284611  0.074710   \n",
      "condition     -0.023783  0.036392  0.028472  -0.124982    -0.058753 -0.008958   \n",
      "grade          0.008130  0.667463  0.356967   0.664983     0.762704  0.113621   \n",
      "sqft_above    -0.010842  0.605566  0.477600   0.685342     0.876597  0.183512   \n",
      "sqft_basement -0.005151  0.323837  0.303093   0.283770     0.435043  0.015286   \n",
      "yr_built       0.021380  0.053982  0.154178   0.506019     0.318049  0.053080   \n",
      "yr_renovated  -0.016907  0.126442  0.018841   0.050739     0.055363  0.007644   \n",
      "zipcode       -0.008224 -0.053168 -0.152668  -0.203866    -0.199430 -0.129574   \n",
      "lat           -0.001891  0.306919 -0.008931   0.024573     0.052529 -0.085683   \n",
      "long           0.020799  0.021571  0.129473   0.223042     0.240223  0.229521   \n",
      "sqft_living15 -0.002901  0.585374  0.391638   0.568634     0.756420  0.144608   \n",
      "sqft_lot15    -0.138798  0.082456  0.029244   0.087175     0.183286  0.718557   \n",
      "\n",
      "                 floors  waterfront      view  condition     grade  \\\n",
      "id             0.018525   -0.002721  0.011592  -0.023783  0.008130   \n",
      "price          0.256786    0.266331  0.397346   0.036392  0.667463   \n",
      "bedrooms       0.175429   -0.006582  0.079532   0.028472  0.356967   \n",
      "bathrooms      0.500653    0.063744  0.187737  -0.124982  0.664983   \n",
      "sqft_living    0.353949    0.103818  0.284611  -0.058753  0.762704   \n",
      "sqft_lot      -0.005201    0.021604  0.074710  -0.008958  0.113621   \n",
      "floors         1.000000    0.023698  0.029444  -0.263768  0.458183   \n",
      "waterfront     0.023698    1.000000  0.401857   0.016653  0.082775   \n",
      "view           0.029444    0.401857  1.000000   0.045990  0.251321   \n",
      "condition     -0.263768    0.016653  0.045990   1.000000 -0.144674   \n",
      "grade          0.458183    0.082775  0.251321  -0.144674  1.000000   \n",
      "sqft_above     0.523885    0.072075  0.167649  -0.158214  0.755923   \n",
      "sqft_basement -0.245705    0.080588  0.276947   0.174105  0.168392   \n",
      "yr_built       0.489319   -0.026161 -0.053440  -0.361417  0.446963   \n",
      "yr_renovated   0.006338    0.092885  0.103917  -0.060618  0.014414   \n",
      "zipcode       -0.059121    0.030285  0.084827   0.003026 -0.184862   \n",
      "lat            0.049614   -0.014274  0.006157  -0.014941  0.114084   \n",
      "long           0.125419   -0.041910 -0.078400  -0.106500  0.198372   \n",
      "sqft_living15  0.279885    0.086463  0.280439  -0.092824  0.713202   \n",
      "sqft_lot15    -0.011269    0.030703  0.072575  -0.003406  0.119248   \n",
      "\n",
      "               sqft_above  sqft_basement  yr_built  yr_renovated   zipcode  \\\n",
      "id              -0.010842      -0.005151  0.021380     -0.016907 -0.008224   \n",
      "price            0.605566       0.323837  0.053982      0.126442 -0.053168   \n",
      "bedrooms         0.477600       0.303093  0.154178      0.018841 -0.152668   \n",
      "bathrooms        0.685342       0.283770  0.506019      0.050739 -0.203866   \n",
      "sqft_living      0.876597       0.435043  0.318049      0.055363 -0.199430   \n",
      "sqft_lot         0.183512       0.015286  0.053080      0.007644 -0.129574   \n",
      "floors           0.523885      -0.245705  0.489319      0.006338 -0.059121   \n",
      "waterfront       0.072075       0.080588 -0.026161      0.092885  0.030285   \n",
      "view             0.167649       0.276947 -0.053440      0.103917  0.084827   \n",
      "condition       -0.158214       0.174105 -0.361417     -0.060618  0.003026   \n",
      "grade            0.755923       0.168392  0.446963      0.014414 -0.184862   \n",
      "sqft_above       1.000000      -0.051943  0.423898      0.023285 -0.261190   \n",
      "sqft_basement   -0.051943       1.000000 -0.133124      0.071323  0.074845   \n",
      "yr_built         0.423898      -0.133124  1.000000     -0.224874 -0.346869   \n",
      "yr_renovated     0.023285       0.071323 -0.224874      1.000000  0.064357   \n",
      "zipcode         -0.261190       0.074845 -0.346869      0.064357  1.000000   \n",
      "lat             -0.000816       0.110538 -0.148122      0.029398  0.267048   \n",
      "long             0.343803      -0.144765  0.409356     -0.068372 -0.564072   \n",
      "sqft_living15    0.731870       0.200355  0.326229     -0.002673 -0.279033   \n",
      "sqft_lot15       0.194050       0.017276  0.070958      0.007854 -0.147221   \n",
      "\n",
      "                    lat      long  sqft_living15  sqft_lot15  \n",
      "id            -0.001891  0.020799      -0.002901   -0.138798  \n",
      "price          0.306919  0.021571       0.585374    0.082456  \n",
      "bedrooms      -0.008931  0.129473       0.391638    0.029244  \n",
      "bathrooms      0.024573  0.223042       0.568634    0.087175  \n",
      "sqft_living    0.052529  0.240223       0.756420    0.183286  \n",
      "sqft_lot      -0.085683  0.229521       0.144608    0.718557  \n",
      "floors         0.049614  0.125419       0.279885   -0.011269  \n",
      "waterfront    -0.014274 -0.041910       0.086463    0.030703  \n",
      "view           0.006157 -0.078400       0.280439    0.072575  \n",
      "condition     -0.014941 -0.106500      -0.092824   -0.003406  \n",
      "grade          0.114084  0.198372       0.713202    0.119248  \n",
      "sqft_above    -0.000816  0.343803       0.731870    0.194050  \n",
      "sqft_basement  0.110538 -0.144765       0.200355    0.017276  \n",
      "yr_built      -0.148122  0.409356       0.326229    0.070958  \n",
      "yr_renovated   0.029398 -0.068372      -0.002673    0.007854  \n",
      "zipcode        0.267048 -0.564072      -0.279033   -0.147221  \n",
      "lat            1.000000 -0.135512       0.048858   -0.086419  \n",
      "long          -0.135512  1.000000       0.334605    0.254451  \n",
      "sqft_living15  0.048858  0.334605       1.000000    0.183192  \n",
      "sqft_lot15    -0.086419  0.254451       0.183192    1.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#warnings.filterwarnings('ignore')\n",
    "datas=pd.read_csv(\"kc_house_data.csv\")\n",
    "\n",
    "X = datas.drop(\"date\",axis=1)\n",
    "X = X.drop(\"price\",axis=1)\n",
    "X = X.drop(\"id\",axis=1)\n",
    "X = X.drop(\"zipcode\",axis=1).values\n",
    "\n",
    "y=datas.iloc[:,2].values\n",
    "corr = datas.corr()\n",
    "print(corr)\n",
    "scaler = MinMaxScaler()\n",
    "X=scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21613, 17)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16209, 17)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n",
      "10\n",
      "20\n",
      "50\n",
      "scores of cross validation:  [-0.0002590466551776416, 0.06724656973594925, 8.119173638243549e-05, -0.00025902284277674604, 0.1253843719821473]\n",
      "accuracy of mlp regression 0.03843881279130492\n",
      "rmse for mlp regression: 374419.63081241504\n",
      "mae for mlp regression: 231670.7351723432\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "    \n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "rmse_mlp=[] \n",
    "mae_mlp=[]\n",
    "accuracy=[]\n",
    "\n",
    "list_1 = [1,5,10,20,50]\n",
    "    #if we use the sigmoid function as our  activation function, we do not have an efficient model.\n",
    "    #the same for tanh\n",
    "    #as learning rate i used the default value, and max_iter=700\n",
    "    #if we choose as solver=lbfgs we take imbalanced results from negative to too good(75%)\n",
    "for i in list_1:\n",
    "    print(i)\n",
    "    regr = MLPRegressor(hidden_layer_sizes=i, activation='tanh', solver='lbfgs',alpha=0.01,learning_rate_init=0.001,max_iter=50000)\n",
    "    regr.fit(X_train, y_train)\n",
    "    scores = cross_val_score(regr, X_train, y_train, cv=5)\n",
    "    accuracy.append(scores.mean())\n",
    "    y_pr = regr.predict((X_test))\n",
    "    error_rmse=rmse(y_pr, y_test)\n",
    "    rmse_mlp.append(error_rmse)\n",
    "    error_mae=mean_absolute_error(y_test, y_pr)\n",
    "    mae_mlp.append(error_mae)\n",
    "\n",
    "\n",
    "print(\"scores of cross validation: \",accuracy) \n",
    "print(\"accuracy of mlp regression\",sum(accuracy)/len(accuracy))    \n",
    "print(\"rmse for mlp regression:\",sum(rmse_mlp) / len(rmse_mlp)) \n",
    "print(\"mae for mlp regression:\",sum(mae_mlp) / len(mae_mlp))     \n",
    "#y_predict = regr.predict(x_test)\n",
    "#score5 = metrics.r2_score(y_test,y_predict)\n",
    "#score5\n",
    "accuracy1=sum(accuracy)/len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03843881279130492"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy1=sum(accuracy)/len(accuracy)\n",
    "accuracy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7479597557649146"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtree_reg = DecisionTreeRegressor(random_state=0)\n",
    "dtree_reg.fit(X_train,y_train)\n",
    "y_pr_dtree = dtree_reg.predict((X_test))\n",
    "#calculation of prediction score\n",
    "from sklearn import metrics\n",
    "accuracy2 = metrics.r2_score(y_test,y_pr_dtree)\n",
    "accuracy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "464/464 [==============================] - 2s 2ms/step - loss: 424381882702.7269\n",
      "Epoch 2/150\n",
      "464/464 [==============================] - 1s 2ms/step - loss: 424931051550.8301\n",
      "Epoch 3/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 429047447058.7183\n",
      "Epoch 4/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 419608406346.3226\n",
      "Epoch 5/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 418657445513.6344\n",
      "Epoch 6/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 425712381537.9957\n",
      "Epoch 7/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 408311521790.8989\n",
      "Epoch 8/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 387700985882.4258\n",
      "Epoch 9/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 398704834846.2795\n",
      "Epoch 10/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 367547113806.7269\n",
      "Epoch 11/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 363625880137.7720\n",
      "Epoch 12/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 351677810331.2516\n",
      "Epoch 13/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 324869286183.0882\n",
      "Epoch 14/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 323596613565.9355\n",
      "Epoch 15/150\n",
      "464/464 [==============================] - 1s 2ms/step - loss: 288789716516.3355\n",
      "Epoch 16/150\n",
      "464/464 [==============================] - 1s 2ms/step - loss: 274389341842.4430\n",
      "Epoch 17/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 256214592170.6667\n",
      "Epoch 18/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 232585009068.3183\n",
      "Epoch 19/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 225034438369.7204\n",
      "Epoch 20/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 194046583330.1333\n",
      "Epoch 21/150\n",
      "464/464 [==============================] - 1s 2ms/step - loss: 172823718940.6280\n",
      "Epoch 22/150\n",
      "464/464 [==============================] - 1s 2ms/step - loss: 160924513603.7161\n",
      "Epoch 23/150\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 146543899513.6688\n",
      "Epoch 24/150\n",
      "464/464 [==============================] - 1s 2ms/step - loss: 128907783126.1591\n",
      "Epoch 25/150\n",
      "464/464 [==============================] - 1s 2ms/step - loss: 140031627237.5742\n",
      "Epoch 26/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 110083772812.3871\n",
      "Epoch 27/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 107593435757.0065\n",
      "Epoch 28/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 105953231288.4301\n",
      "Epoch 29/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 97566434618.9075\n",
      "Epoch 30/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 98828642361.2559\n",
      "Epoch 31/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 100613147725.0753\n",
      "Epoch 32/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 91927837319.4323\n",
      "Epoch 33/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 96636991351.4667\n",
      "Epoch 34/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 87927362152.6022\n",
      "Epoch 35/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 89301616063.0366\n",
      "Epoch 36/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 96691462628.4731\n",
      "Epoch 37/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 87902670777.5312\n",
      "Epoch 38/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 97806763446.2280\n",
      "Epoch 39/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 87772260735.1742\n",
      "Epoch 40/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 83675073996.2495\n",
      "Epoch 41/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 96212974735.1398\n",
      "Epoch 42/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 97932373540.3355\n",
      "Epoch 43/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 99182051187.0624\n",
      "Epoch 44/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 94288015219.0624\n",
      "Epoch 45/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 85223148590.2452\n",
      "Epoch 46/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 86273455454.1419\n",
      "Epoch 47/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 93084295176.8086\n",
      "Epoch 48/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 87744099413.8839\n",
      "Epoch 49/150\n",
      "464/464 [==============================] - 1s 3ms/step - loss: 93942178320.5161A: 0s - loss: 9\n",
      "Epoch 50/150\n",
      "464/464 [==============================] - 1s 2ms/step - loss: 90553135720.6022\n",
      "Epoch 51/150\n",
      "464/464 [==============================] - 1s 2ms/step - loss: 88543756318.8301\n",
      "Epoch 52/150\n",
      "464/464 [==============================] - 1s 2ms/step - loss: 83973695778.6839\n",
      "Epoch 53/150\n",
      "464/464 [==============================] - 1s 2ms/step - loss: 87627803641.3936\n",
      "Epoch 54/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 83255610475.9054\n",
      "Epoch 55/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 89503912085.7462\n",
      "Epoch 56/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 82517699511.3290\n",
      "Epoch 57/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 83534107284.6452\n",
      "Epoch 58/150\n",
      "464/464 [==============================] - 1s 2ms/step - loss: 89448339583.7247\n",
      "Epoch 59/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 76065573969.4796\n",
      "Epoch 60/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 89718680146.5806\n",
      "Epoch 61/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 83131469489.2731\n",
      "Epoch 62/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 78154421052.0086\n",
      "Epoch 63/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 79553173904.7914\n",
      "Epoch 64/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 80702411359.7935\n",
      "Epoch 65/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 91987040084.2323\n",
      "Epoch 66/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 86932444818.4430\n",
      "Epoch 67/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 75074381986.9591\n",
      "Epoch 68/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 81955516759.5355\n",
      "Epoch 69/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 84903567166.2108\n",
      "Epoch 70/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 78441341068.9376\n",
      "Epoch 71/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 81045136483.0968\n",
      "Epoch 72/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 84291237161.2903\n",
      "Epoch 73/150\n",
      "464/464 [==============================] - 1s 2ms/step - loss: 80450667451.7333\n",
      "Epoch 74/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 76607629334.0215\n",
      "Epoch 75/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 78320160688.7226\n",
      "Epoch 76/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 67644164364.6624\n",
      "Epoch 77/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 75796705808.5161\n",
      "Epoch 78/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 77241280287.3806\n",
      "Epoch 79/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 71262773970.3054\n",
      "Epoch 80/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 74658520356.8860\n",
      "Epoch 81/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 78414591475.8882\n",
      "Epoch 82/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 71035550140.8344\n",
      "Epoch 83/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 83762168732.9032\n",
      "Epoch 84/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 75334807005.8667\n",
      "Epoch 85/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 75422219627.3548\n",
      "Epoch 86/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 67560935397.5742\n",
      "Epoch 87/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 67026211895.0538\n",
      "Epoch 88/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 65936727892.2323\n",
      "Epoch 89/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 71156826605.2817\n",
      "Epoch 90/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 71392749572.4043\n",
      "Epoch 91/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 64922604515.3720\n",
      "Epoch 92/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 72674179666.5806\n",
      "Epoch 93/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 67121772055.1226\n",
      "Epoch 94/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 66150778690.6151\n",
      "Epoch 95/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 65837962713.4624\n",
      "Epoch 96/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 66849959900.7656\n",
      "Epoch 97/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 72568628565.3333\n",
      "Epoch 98/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 68199134373.1613\n",
      "Epoch 99/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 64525466652.6280\n",
      "Epoch 100/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 65131688323.5785\n",
      "Epoch 101/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 67586999883.9742\n",
      "Epoch 102/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 69856438611.1312\n",
      "Epoch 103/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 66407228400.5849\n",
      "Epoch 104/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 62119103648.7570\n",
      "Epoch 105/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 62170650258.4430\n",
      "Epoch 106/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 58559416725.1957\n",
      "Epoch 107/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 61061634114.0645\n",
      "Epoch 108/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 71872806101.6086\n",
      "Epoch 109/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 60245124842.5290\n",
      "Epoch 110/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 60991508435.9570\n",
      "Epoch 111/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 69933903264.2065\n",
      "Epoch 112/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 63994011650.2021\n",
      "Epoch 113/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 57089163783.7075\n",
      "Epoch 114/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 61061870131.7505\n",
      "Epoch 115/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 62454691245.4194\n",
      "Epoch 116/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 57430719970.2710\n",
      "Epoch 117/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 57478998020.4043\n",
      "Epoch 118/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 55910906622.3484\n",
      "Epoch 119/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 57445989933.1441\n",
      "Epoch 120/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 62476075122.5118\n",
      "Epoch 121/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 57906143734.0903\n",
      "Epoch 122/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 60222115276.2495\n",
      "Epoch 123/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 53665883199.8624\n",
      "Epoch 124/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 54347203960.5677\n",
      "Epoch 125/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 58833004028.6968\n",
      "Epoch 126/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 61134369375.7935\n",
      "Epoch 127/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 55625520883.3376\n",
      "Epoch 128/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 57137204437.6086\n",
      "Epoch 129/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 51543259519.1742\n",
      "Epoch 130/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 67186124293.5054\n",
      "Epoch 131/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 54604330386.9935\n",
      "Epoch 132/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 56516318249.8409\n",
      "Epoch 133/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 55440500713.9785\n",
      "Epoch 134/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 52159673297.7548\n",
      "Epoch 135/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 57674802671.4839\n",
      "Epoch 136/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 54429346492.2839\n",
      "Epoch 137/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 55790394374.6065\n",
      "Epoch 138/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 51055552335.8280\n",
      "Epoch 139/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 53515654899.3376\n",
      "Epoch 140/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 54666045272.6366\n",
      "Epoch 141/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 51986066901.0581\n",
      "Epoch 142/150\n",
      "464/464 [==============================] - 0s 1ms/step - loss: 56088727877.9183\n",
      "Epoch 143/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 56103874075.5269\n",
      "Epoch 144/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 49049196880.9290\n",
      "Epoch 145/150\n",
      "464/464 [==============================] - 1s 2ms/step - loss: 47897869851.5269\n",
      "Epoch 146/150\n",
      "464/464 [==============================] - 1s 2ms/step - loss: 50911304063.1742\n",
      "Epoch 147/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 51811939246.5204\n",
      "Epoch 148/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 47013021392.1032\n",
      "Epoch 149/150\n",
      "464/464 [==============================] - 1s 1ms/step - loss: 51270510642.6495\n",
      "Epoch 150/150\n",
      "464/464 [==============================] - 1s 2ms/step - loss: 54926968158.1419\n",
      "Accuracy of Keras Model: 0.6075648994383837\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(10, activation = 'relu', input_dim = 17))\n",
    "model.add(Dense(5, activation = 'relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer = 'adam',loss = 'mean_squared_error')\n",
    "model.fit(X_train, y_train, batch_size = 35, epochs = 150)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy3 = metrics.r2_score(y_test,y_pred)\n",
    "print(\"Accuracy of Keras Model:\",accuracy3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We continue our analysis by implementing 3 different regression techniques on our second dataset.First we used sklearn in order to apply mlp regressor to our dataset.We took a really low average accuracy close to 3.8% with 5 fold cross validation and for 5 different sizes of hidden layers.\n",
    "# Furthermore,from sklearn library, we used decision tree regressor and we ended up with an acceptable accuracy of 75%.\n",
    "# And finally our keras implementation gave 60% accuracy with 2 hidden layers with 10 and 5 neurons respectively.\n",
    "# So, we observe that decision tree regressor gave better accuracy than the other 2 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7825, 0.8731249999999999, 0.7375, 0.8625]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [score1,score2,score3,score4]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#comparison diagram between the score of each method according to the regression applied.\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1.8,1.8])\n",
    "algorithms = ['Numpy Model', 'Sklearn MLP Model', 'Keras Model','Decision Trees']\n",
    "total_scores= scores\n",
    "ax.bar(algorithms,total_scores)\n",
    "plt.title('PREDICTION SCORE COMPARISON')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Classification Model\")\n",
    "plt.ylabel(\"PREDICTION SCORE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.03843881279130492, 0.7479597557649146, 0.6075648994383837]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = [accuracy1,accuracy2,accuracy3]\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1.8,1.8])\n",
    "algorithms = ['Sklearn MLP Model','Decision Trees','Keras Model']\n",
    "total_scores= accuracy\n",
    "ax.bar(algorithms,total_scores)\n",
    "plt.title('PREDICTION SCORE COMPARISON')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Regression Model\")\n",
    "plt.ylabel(\"PREDICTION SCORE\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
